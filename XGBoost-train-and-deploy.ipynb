{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "90fd0552-c40e-4f9f-bb1e-eb7fd7562418",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip3 install --upgrade xgboost google-cloud-aiplatform --user -q --no-warn-script-location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937c329c-5649-41c7-8b55-b8a66efb9f2e",
   "metadata": {},
   "source": [
    "## 1. Get Project ID and Create Bucket\n",
    "\n",
    "\\[This will issues a ServiceException if a bucket is already there with the name. Should omit the exception in that case\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3f95c924-6b3d-4909-ad2a-b4618a79d25b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: quixotic-galaxy-439523-m3\n",
      "Bucket Name: quixotic-galaxy-439523-m3-buckt\n",
      "Creating gs://quixotic-galaxy-439523-m3-buckt/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'quixotic-galaxy-439523-m3-buckt' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID_DETAILS = !gcloud config get-value project\n",
    "PROJECT_ID = PROJECT_ID_DETAILS[0]  # The project ID is item 0 in the list returned by the gcloud command\n",
    "BUCKET=f\"{PROJECT_ID}-buckt\"\n",
    "REGION=\"us-central1\"\n",
    "print(f\"Project ID: {PROJECT_ID}\")\n",
    "print(f\"Bucket Name: {BUCKET}\")\n",
    "\n",
    "!gsutil mb -l {REGION} gs://{BUCKET}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729f5983-6431-47a5-b419-b72d5198e39b",
   "metadata": {},
   "source": [
    "## 2. Set Bucket URI and Other Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2bf70d22-ebba-4144-827c-caa55e23fa7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUCKET_URI=f\"gs://{BUCKET}\"\n",
    "APP_NAME=\"boston-housing\"\n",
    "APPLICATION_DIR = \"hypo_opt\"\n",
    "TRAINER_DIR = f\"{APPLICATION_DIR}/trainer\"\n",
    "\n",
    "!mkdir -p $APPLICATION_DIR\n",
    "!mkdir -p $TRAINER_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997771cb-89a6-4bef-b2a6-904911fd0e8f",
   "metadata": {},
   "source": [
    "## 3. Download the Dataset, Clean it and Save in the Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "61d3f439-52ae-41f5-aee1-0165c6994639",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://boston_housing.csv [Content-Type=text/csv]...\n",
      "/ [1 files][ 38.2 KiB/ 38.2 KiB]                                                \n",
      "Operation completed over 1 objects/38.2 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]\n",
    "target = target.reshape((len(target)),1)\n",
    "\n",
    "column_names =  [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\", \"MEDV\"]\n",
    "data_df = pd.DataFrame(np.concatenate((data, target), axis=1), columns=column_names)\n",
    "\n",
    "file_path = 'boston_housing.csv'\n",
    "data_df.to_csv(file_path, index=False)\n",
    "!gsutil cp boston_housing.csv $BUCKET_URI/boston_housing.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b898ebb-42f7-46b1-ac88-c46e00b3e25e",
   "metadata": {},
   "source": [
    "## 4. Containerize the Training Application Code\n",
    "\n",
    "### 4.1 Initialize AI Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7d2ac61a-bfe3-4bb3-9456-13bfdd5e56de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "\n",
    "# Initialize the AI Platform client\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eed0a6b-7214-475d-b92b-22381d098f9a",
   "metadata": {},
   "source": [
    "### 4.2 Set Pre-Built Containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1a74a33a-aae5-44a0-ab2a-e10da09a0a25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_VERSION = \"xgboost-cpu.1-1\"\n",
    "DEPLOY_VERSION = \"xgboost-cpu.1-1\"\n",
    "\n",
    "TRAIN_IMAGE = \"{}-docker.pkg.dev/vertex-ai/training/{}:latest\".format(\n",
    "    REGION.split(\"-\")[0], TRAIN_VERSION\n",
    ")\n",
    "DEPLOY_IMAGE = \"{}-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(\n",
    "    REGION.split(\"-\")[0], DEPLOY_VERSION\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e043b22-ca17-46e5-847f-fc35e2b09b83",
   "metadata": {},
   "source": [
    "### 4.3 Create a Folder Structure as Python Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7650d8b8-5d4b-42b5-b4b3-82a249eac201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make folder for Python training script\n",
    "! rm -rf custom\n",
    "! mkdir custom\n",
    "\n",
    "# Add package information\n",
    "! touch custom/README.md\n",
    "\n",
    "setup_cfg = \"[egg_info]\\n\\ntag_build =\\n\\ntag_date = 0\"\n",
    "! echo \"$setup_cfg\" > custom/setup.cfg\n",
    "\n",
    "setup_py = \"import setuptools\\n\\nsetuptools.setup(\\n\\n    install_requires=[\\n\\n        'cloudml-hypertune', 'gcsfs' \\n\\n    ],\\n\\n    packages=setuptools.find_packages())\"\n",
    "! echo \"$setup_py\" > custom/setup.py\n",
    "\n",
    "pkg_info = \"Metadata-Version: 1.0\\n\\nName: Iris tabular classification\\n\\nVersion: 0.0.0\\n\\nSummary: Demostration training script\\n\\nHome-page: www.google.com\\n\\nAuthor: Google\\n\\nAuthor-email: aferlitsch@google.com\\n\\nLicense: Public\\n\\nDescription: Demo\\n\\nPlatform: Vertex\"\n",
    "! echo \"$pkg_info\" > custom/PKG-INFO\n",
    "\n",
    "# Make the training subfolder\n",
    "! mkdir custom/trainer\n",
    "! touch custom/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c94f01-0515-423d-a51b-0d1941478220",
   "metadata": {},
   "source": [
    "### 4.4 Create Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a8bdd12b-0121-4fa6-beb8-3640a0f3d061",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing custom/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom/trainer/task.py\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from hypertune import HyperTune\n",
    "import os\n",
    "import logging\n",
    "\n",
    "data_location=\"gs://quixotic-galaxy-439523-m3-buckt/boston_housing.csv\"\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "def train_model(data, n_estimators, max_depth, learning_rate, subsample):\n",
    "    \n",
    "    y = data.pop(\"MEDV\")\n",
    "    X = data\n",
    "    \n",
    "    # Split the data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_depth': max_depth,\n",
    "        'learning_rate': learning_rate,\n",
    "        'subsample': subsample\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    model.fit(X_train, y_train, verbose=False)\n",
    "    \n",
    "    mmse_val = mean_squared_error(y_val, model.predict(X_val))\n",
    "\n",
    "    hpt = HyperTune()\n",
    "    #hpt.report_hyperparameter_tuning_metric(\n",
    "    #    hyperparameter_metric_tag='mmse_val',\n",
    "    #    metric_value=mmse_val,\n",
    "    #    global_step=1000)\n",
    "    hpt.report_hyperparameter_tuning_metric(\n",
    "        hyperparameter_metric_tag='mmse_val',\n",
    "        metric_value=mmse_val)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description='XGBoost Hyperparameter Tuning')\n",
    "    parser.add_argument('--n_estimators', type=int, default=10)\n",
    "    parser.add_argument('--max_depth', type=int, default=5)\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.1)\n",
    "    parser.add_argument('--subsample', type=float, default=0.1)\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def main():\n",
    "    args = get_args()\n",
    "    data = pd.read_csv(data_location)\n",
    "    model = train_model(data=data, \n",
    "                        n_estimators=args.n_estimators, \n",
    "                        max_depth=args.max_depth,\n",
    "                        learning_rate=args.learning_rate, \n",
    "                        subsample=args.subsample)\n",
    "    \n",
    "    model_dir = os.getenv('AIP_MODEL_DIR')\n",
    "    # GCSFuse conversion\n",
    "    gs_prefix = 'gs://'\n",
    "    gcsfuse_prefix = '/gcs/'\n",
    "    if model_dir.startswith(gs_prefix):\n",
    "        model_dir = model_dir.replace(gs_prefix, gcsfuse_prefix)\n",
    "        dirpath = os.path.split(model_dir)[0]\n",
    "        if not os.path.isdir(dirpath):\n",
    "            os.makedirs(dirpath)\n",
    "    \n",
    "    gcs_model_path = os.path.join(model_dir, 'model.bst')\n",
    "    model.save_model(gcs_model_path)\n",
    "    logging.info(f\"Saved model artifacts to {gcs_model_path}\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c5eac2-a7cb-4dc4-92e1-3e08f3d52c4c",
   "metadata": {},
   "source": [
    "### 4.5 Store Training Script on Cloud Storage Bucket\n",
    "\n",
    "Compress the whole training folder as a tar ball and then store it in a Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3e8094cc-2f9e-4d23-9131-3949e1ad687a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom/\n",
      "custom/PKG-INFO\n",
      "custom/setup.py\n",
      "custom/trainer/\n",
      "custom/trainer/__init__.py\n",
      "custom/trainer/task.py\n",
      "custom/README.md\n",
      "custom/setup.cfg\n",
      "Copying file://custom.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  1.5 KiB/  1.5 KiB]                                                \n",
      "Operation completed over 1 objects/1.5 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "! rm -f custom.tar custom.tar.gz\n",
    "! tar cvf custom.tar custom\n",
    "! gzip custom.tar\n",
    "! gsutil cp custom.tar.gz $BUCKET_URI/trainer_iris.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff5381d-fbc8-4fce-88e6-62dbb11a2f0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Hyperparameter Tuining Job Setup\n",
    "\n",
    "### 5.1 Set Worker Pool Specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6a9fabdf-cf51-42b7-9aed-1f877a6bbb53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "worker_pool_spec = [\n",
    "    {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\",\n",
    "        },\n",
    "        \"python_package_spec\": {\n",
    "            \"executor_image_uri\": TRAIN_IMAGE,\n",
    "            \"package_uris\": [BUCKET_URI + \"/trainer_iris.tar.gz\"],\n",
    "            \"python_module\": \"trainer.task\",\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0bd76681-75de-4da0-9e25-7cdf2336bee5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define custom job\n",
    "MODEL_DIR = f\"{BUCKET_URI}/aiplatform-custom-job\"\n",
    "\n",
    "custom_job = aiplatform.CustomJob(\n",
    "    display_name=\"xgboost_train\",\n",
    "    worker_pool_specs=worker_pool_spec,\n",
    "    base_output_dir=MODEL_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "008f96e6-ce0e-4937-9acb-bcc367676751",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the hyperparameter tuning spec\n",
    "hpt_job = aiplatform.HyperparameterTuningJob(\n",
    "    display_name=\"xgboost_hpt\",\n",
    "    custom_job=custom_job,\n",
    "    metric_spec={\n",
    "        \"mmse_val\": \"minimize\",\n",
    "    },\n",
    "    parameter_spec={\n",
    "        \"n_estimators\": aiplatform.hyperparameter_tuning.IntegerParameterSpec(min=3, max=4, scale='linear'),\n",
    "        \"max_depth\": aiplatform.hyperparameter_tuning.IntegerParameterSpec(min=3, max=4, scale='linear'),\n",
    "        \"learning_rate\": aiplatform.hyperparameter_tuning.DoubleParameterSpec(min=0.1, max=0.2, scale='log'),\n",
    "        \"subsample\": aiplatform.hyperparameter_tuning.DoubleParameterSpec(min=0.1, max=0.2, scale='log'),\n",
    "    },\n",
    "    max_trial_count=5, #This was to limit the time taken for tuning and save free credit. Should be a larger value ideally\n",
    "    parallel_trial_count=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94214c4f-ae82-4565-bd6f-dde6d5eb271f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating HyperparameterTuningJob\n",
      "HyperparameterTuningJob created. Resource name: projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080\n",
      "To use this HyperparameterTuningJob in another session:\n",
      "hpt_job = aiplatform.HyperparameterTuningJob.get('projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080')\n",
      "View HyperparameterTuningJob:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/582446871562158080?project=442313554841\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "HyperparameterTuningJob run completed. Resource name: projects/442313554841/locations/us-central1/hyperparameterTuningJobs/582446871562158080\n"
     ]
    }
   ],
   "source": [
    "hpt_job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ca3b63-caac-45b1-98a7-8e4e1e3aee56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://quixotic-galaxy-439523-m3-buckt/aiplatform-custom-job/5/model/\n",
      "gs://quixotic-galaxy-439523-m3-buckt/aiplatform-custom-job/5/model/model.bst\n",
      "gs://quixotic-galaxy-439523-m3-buckt/aiplatform-custom-job/5/model/model.pkl\n"
     ]
    }
   ],
   "source": [
    "results = np.array([trail.final_measurement.metrics[0].value for trail in hpt_job.trials])\n",
    "id_min = results.argmin()\n",
    "BEST_MODEL_DIR = MODEL_DIR + \"/\" + hpt_job.trials[id_min].id + \"/model\"\n",
    "! gsutil ls {BEST_MODEL_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad28947-2d40-4882-992a-c981fc80a2d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/442313554841/locations/us-central1/models/5726121317527191552/operations/2330540960951304192\n"
     ]
    },
    {
     "ename": "FailedPrecondition",
     "evalue": "400 Model directory gs://quixotic-galaxy-439523-m3-buckt/aiplatform-custom-job/5/model is expected to contain exactly one of: [model.pkl, model.joblib, model.bst].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPrecondition\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_v1 \u001b[38;5;241m=\u001b[39m \u001b[43maiplatform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplay_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxgboost_best_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43martifact_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBEST_MODEL_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserving_container_image_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEPLOY_IMAGE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_default_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(model_v1)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/base.py:863\u001b[0m, in \u001b[0;36moptional_sync.<locals>.optional_run_in_thread.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m    862\u001b[0m         VertexAiResourceNounWithFutureManager\u001b[38;5;241m.\u001b[39mwait(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;66;03m# callbacks to call within the Future (in same Thread)\u001b[39;00m\n\u001b[1;32m    866\u001b[0m internal_callbacks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/models.py:5041\u001b[0m, in \u001b[0;36mModel.upload\u001b[0;34m(cls, serving_container_image_uri, artifact_uri, model_id, parent_model, is_default_version, version_aliases, version_description, serving_container_predict_route, serving_container_health_route, description, serving_container_command, serving_container_args, serving_container_environment_variables, serving_container_ports, serving_container_grpc_ports, local_model, instance_schema_uri, parameters_schema_uri, prediction_schema_uri, explanation_metadata, explanation_parameters, display_name, project, location, credentials, labels, encryption_spec_key_name, staging_bucket, sync, upload_request_timeout, serving_container_deployment_timeout, serving_container_shared_memory_size_mb, serving_container_startup_probe_exec, serving_container_startup_probe_period_seconds, serving_container_startup_probe_timeout_seconds, serving_container_health_probe_exec, serving_container_health_probe_period_seconds, serving_container_health_probe_timeout_seconds)\u001b[0m\n\u001b[1;32m   5034\u001b[0m lro \u001b[38;5;241m=\u001b[39m api_client\u001b[38;5;241m.\u001b[39mupload_model(\n\u001b[1;32m   5035\u001b[0m     request\u001b[38;5;241m=\u001b[39mrequest,\n\u001b[1;32m   5036\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mupload_request_timeout,\n\u001b[1;32m   5037\u001b[0m )\n\u001b[1;32m   5039\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mlog_create_with_lro(\u001b[38;5;28mcls\u001b[39m, lro)\n\u001b[0;32m-> 5041\u001b[0m model_upload_response \u001b[38;5;241m=\u001b[39m \u001b[43mlro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5043\u001b[0m this_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m   5044\u001b[0m     model_upload_response\u001b[38;5;241m.\u001b[39mmodel, version\u001b[38;5;241m=\u001b[39mmodel_upload_response\u001b[38;5;241m.\u001b[39mmodel_version_id\n\u001b[1;32m   5045\u001b[0m )\n\u001b[1;32m   5047\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mlog_create_complete(\u001b[38;5;28mcls\u001b[39m, this_model\u001b[38;5;241m.\u001b[39m_gca_resource, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/future/polling.py:261\u001b[0m, in \u001b[0;36mPollingFuture.result\u001b[0;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking_poll(timeout\u001b[38;5;241m=\u001b[39mtimeout, retry\u001b[38;5;241m=\u001b[39mretry, polling\u001b[38;5;241m=\u001b[39mpolling)\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# pylint: disable=raising-bad-type\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# Pylint doesn't recognize that this is valid in this case.\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "\u001b[0;31mFailedPrecondition\u001b[0m: 400 Model directory gs://quixotic-galaxy-439523-m3-buckt/aiplatform-custom-job/5/model is expected to contain exactly one of: [model.pkl, model.joblib, model.bst]."
     ]
    }
   ],
   "source": [
    "model_v1 = aiplatform.Model.upload(\n",
    "    display_name=\"xgboost_best_model\",\n",
    "    artifact_uri=BEST_MODEL_DIR,\n",
    "    serving_container_image_uri=DEPLOY_IMAGE,\n",
    "    is_default_version=True\n",
    ")\n",
    "\n",
    "print(model_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4108ffae-74d4-4413-b069-ee592e5317db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "endpoint = aiplatform.Endpoint.create(\n",
    "    display_name=\"xgboost_model_endpoint\",\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    labels={\"your_key\": \"your_value\"},\n",
    ")\n",
    "\n",
    "print(endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c77101-f880-464a-a3d2-5e1b2aeef294",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = endpoint.deploy(\n",
    "    model=model_v1,\n",
    "    deployed_model_display_name=\"example_\",\n",
    "    machine_type=\"n1-standard-4\",\n",
    ")\n",
    "\n",
    "print(endpoint)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
